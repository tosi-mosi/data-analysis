{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a188c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, BatchNormalization, Dropout, LSTM, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "from cv2 import imread, resize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711f3a9",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b6c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../googleplaystore.csv')\n",
    "row_10472 = df.loc[10472,:]\n",
    "df.loc[10472,:] = [row_10472['App'], row_10472['Content Rating'], row_10472['Category'], row_10472['Rating'], row_10472['Reviews'],\n",
    "                   row_10472['Size'], row_10472['Installs'], row_10472['Type'], row_10472['Price'], row_10472['Android Ver'], \n",
    "                   row_10472['Genres'], row_10472['Last Updated'], row_10472['Current Ver']]\n",
    "\n",
    "df.drop(['Type','Category','Current Ver','Android Ver'],axis=1, inplace=True)\n",
    "\n",
    "lastUpdated = pd.to_datetime(df['Last Updated'])\n",
    "df.loc[:,'Last Updated'] = pd.to_datetime(df.loc[:,'Last Updated'])\n",
    "df.loc[:,'Last Updated'] = df.loc[:,'Last Updated'].map(lambda x: (datetime.today()-x).days)\n",
    "\n",
    "df.loc[:,'Size'] = df.loc[:,'Size'].map(lambda x: float(x[:-1])*1000000 if x[-1] == 'M' else float(x[:-1])*1000 if x[-1] == 'k' else x)\n",
    "\n",
    "#replacing $ in price\n",
    "df['Price'].replace(\n",
    "    {r'\\$([0-9]*\\.*[0-9]*)': r'\\1'},\n",
    "    inplace = True,\n",
    "    regex = True)\n",
    "\n",
    "#replacing , in installs\n",
    "df['Installs'].replace(\n",
    "    {r'([0-9]*)\\,*([0-9]*)\\,*([0-9]*)\\,*([0-9]*)\\+': r'\\1\\2\\3\\4'},\n",
    "    inplace = True,\n",
    "    regex = True)\n",
    "\n",
    "#where varies with device \n",
    "df['Size'].replace(\"Varies with device\", float(\"NaN\"), inplace=True)\n",
    "df.drop(df[pd.to_numeric(df.Size, errors = 'coerce').isnull()].index,\n",
    "                  axis = 0,\n",
    "                  inplace = True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#categorical values encoding (using label encoding https://pbpython.com/categorical-encoding.html#approach-2-label-encoding)\n",
    "#print(df['Genres'].unique())\n",
    "df = df.astype({\n",
    "    'Genres':             'category',\n",
    "    'Content Rating':     'category'\n",
    "})\n",
    "\n",
    "df['Genres Category'] = df['Genres'].cat.codes\n",
    "df['Content Rating Category'] = df['Content Rating'].cat.codes\n",
    "\n",
    "feature_columns = ['Reviews', 'Size', 'Installs',  'Price', 'Content Rating Category', 'Genres Category', 'Last Updated']\n",
    "output_column = ['Rating']\n",
    "relevant_data = df[feature_columns + output_column]\n",
    "relevant_data = relevant_data.apply(pd.to_numeric)\n",
    "\n",
    "#spliting in viewer sympathy in 2 classes by rating\n",
    "relevant_data.Rating = relevant_data.Rating.map(lambda x: 1 if x>=4.0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd0aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    relevant_data[feature_columns], relevant_data[output_column], test_size = 0.5, random_state = 0\n",
    ")\n",
    "\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "X_test = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80c9aff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-05 16:05:59.292729: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-01-05 16:05:59.292773: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-05 16:05:59.292801: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (b2b9b6b183c4): /proc/driver/nvidia/version does not exist\n",
      "2022-01-05 16:05:59.293066: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "97/97 [==============================] - 1s 3ms/step - loss: 0.6100 - accuracy: 0.7318 - val_loss: 0.5467 - val_accuracy: 0.7697\n",
      "Epoch 2/10\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5412 - accuracy: 0.7651 - val_loss: 0.5341 - val_accuracy: 0.7697\n",
      "Epoch 3/10\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5369 - accuracy: 0.7651 - val_loss: 0.5320 - val_accuracy: 0.7697\n",
      "Epoch 4/10\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5348 - accuracy: 0.7648 - val_loss: 0.5303 - val_accuracy: 0.7710\n",
      "Epoch 5/10\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5330 - accuracy: 0.7651 - val_loss: 0.5289 - val_accuracy: 0.7697\n",
      "Epoch 6/10\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5313 - accuracy: 0.7648 - val_loss: 0.5280 - val_accuracy: 0.7697\n",
      "Epoch 7/10\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5299 - accuracy: 0.7658 - val_loss: 0.5267 - val_accuracy: 0.7710\n",
      "Epoch 8/10\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5289 - accuracy: 0.7658 - val_loss: 0.5259 - val_accuracy: 0.7710\n",
      "Epoch 9/10\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5279 - accuracy: 0.7648 - val_loss: 0.5249 - val_accuracy: 0.7710\n",
      "Epoch 10/10\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5271 - accuracy: 0.7654 - val_loss: 0.5243 - val_accuracy: 0.7710\n",
      "\n",
      "Test score: 0.5231268405914307\n",
      "Test accuracy: 0.7673997282981873\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim = len(feature_columns)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2, activation = 'sigmoid'))\n",
    "model.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = 'sgd',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,                                   \n",
    "                    validation_split=0.2,\n",
    "                    verbose = 1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0) \n",
    "print('\\nTest score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb7a5d",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad9001c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_per_celeb = 100\n",
    "path = '../Sports-celebrity-imgs/'\n",
    "dirs = ['Kane Williamson', 'Kobe Bryant', 'Maria Sharapova',]# 'Ronaldo']\n",
    "X = []\n",
    "y = []\n",
    "count = 0\n",
    "for d in dirs:\n",
    "    full_path = path + d\n",
    "    for image in os.listdir(full_path):\n",
    "        img = imread((os.path.join(full_path, image)), 1)[...,::-1] / 255\n",
    "        img = resize(img, (100, 100))\n",
    "        X.append(img)\n",
    "        y.append(count)\n",
    "    count += 1\n",
    "    \n",
    "random.seed(1)\n",
    "random.shuffle(X)\n",
    "# reset the same seed to get the identical random sequence and shuffle the y\n",
    "random.seed(1)\n",
    "random.shuffle(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84612c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 13)\n",
    "X_train = np.array(X_train)\n",
    "y_train = to_categorical(np.array(y_train))\n",
    "X_test = np.array(X_test)\n",
    "y_test = to_categorical(np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f58ca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14/14 [==============================] - 2s 124ms/step - loss: 3.5535 - recall: 0.5000 - accuracy: 0.0126 - val_loss: 1.0448 - val_recall: 0.2453 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 1s 101ms/step - loss: 1.2879 - recall: 0.6368 - accuracy: 0.0094 - val_loss: 0.9512 - val_recall: 0.2830 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 1s 100ms/step - loss: 0.7990 - recall: 0.7264 - accuracy: 0.0063 - val_loss: 0.9757 - val_recall: 0.2642 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 1s 101ms/step - loss: 0.6174 - recall: 0.7547 - accuracy: 0.0063 - val_loss: 1.2264 - val_recall: 0.3962 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 1s 100ms/step - loss: 0.6105 - recall: 0.8160 - accuracy: 0.0142 - val_loss: 1.0728 - val_recall: 0.4151 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 1s 101ms/step - loss: 0.5135 - recall: 0.7925 - accuracy: 0.0126 - val_loss: 2.1959 - val_recall: 0.4340 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 1s 99ms/step - loss: 0.3238 - recall: 0.8585 - accuracy: 0.0189 - val_loss: 1.9044 - val_recall: 0.4340 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 1s 101ms/step - loss: 0.6760 - recall: 0.7689 - accuracy: 0.0157 - val_loss: 1.5222 - val_recall: 0.4528 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 1s 101ms/step - loss: 0.1854 - recall: 0.9434 - accuracy: 0.0204 - val_loss: 1.4787 - val_recall: 0.4906 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 1s 100ms/step - loss: 0.2905 - recall: 0.9009 - accuracy: 0.0425 - val_loss: 3.9720 - val_recall: 0.4340 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Test score: 4.50176477432251\n",
      "Test accuracy: 0.34328359365463257\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "    \n",
    "model.add(Conv2D(32, (3, 3), input_shape = (100, 100, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "model.compile(\n",
    "        loss = 'categorical_crossentropy',\n",
    "        optimizer = 'rmsprop',\n",
    "        metrics = [keras.metrics.Recall(name = 'recall'), \n",
    "                   keras.metrics.Accuracy(name = 'accuracy')]\n",
    "    )\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=16,                                   \n",
    "                    validation_split=0.2,\n",
    "                    verbose = 1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0) \n",
    "print('\\nTest score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f27c1d3",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d4fdd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "df = pd.read_csv('../Corona_NLP_train.csv')\n",
    "df = df[:5000]\n",
    "df_test = pd.read_csv('../Corona_NLP_test.csv')\n",
    "df_test = df_test[:5000]\n",
    "\n",
    "def preprocess_df(df):\n",
    "    df['OriginalTweet'].replace(\n",
    "        {r'[^a-zA-Z\\ ]': r''},\n",
    "        inplace = True,\n",
    "        regex = True)\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    def filter_stop_words(text_tokens):\n",
    "        filtered_text = []\n",
    "        for w in text_tokens:\n",
    "            if w not in stop_words:\n",
    "                filtered_text.append(w)\n",
    "        return filtered_text\n",
    "    \n",
    "    def to_int_category(x):\n",
    "        if x == 'Negative':\n",
    "            return 0\n",
    "        elif x == 'Neutral':\n",
    "            return 1\n",
    "        elif x == 'Positive':\n",
    "            return 2\n",
    "\n",
    "    df.loc[:,'OriginalTweetPreprocessed'] = df.loc[:,'OriginalTweet'].map(lambda x: \" \".join(filter_stop_words(word_tokenize(x))))\n",
    "    df.loc[:,'Sentiment'] = df.loc[:,'Sentiment'].map(lambda x: x.replace('Extremely ', ''))\n",
    "    df.loc[:,'Sentiment'] = df.loc[:,'Sentiment'].map(to_int_category)\n",
    "    # words = word_tokenize(text)\n",
    "#     df = df[['OriginalTweetPreprocessed', 'OriginalTweet', 'Sentiment']]\n",
    "    df.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1, inplace=True)\n",
    "\n",
    "preprocess_df(df)\n",
    "preprocess_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd722d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>OriginalTweetPreprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MeNyrbie PhilGahan Chrisitv httpstcoiFzFAnPa a...</td>\n",
       "      <td>1</td>\n",
       "      <td>MeNyrbie PhilGahan Chrisitv httpstcoiFzFAnPa h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>2</td>\n",
       "      <td>advice Talk neighbours family exchange phone n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia Woolworths to give elder...</td>\n",
       "      <td>2</td>\n",
       "      <td>Coronavirus Australia Woolworths give elderly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>2</td>\n",
       "      <td>My food stock one emptyPLEASE dont panic THERE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me ready to go at supermarket during the COVID...</td>\n",
       "      <td>0</td>\n",
       "      <td>Me ready go supermarket COVID outbreakNot Im p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet  Sentiment  \\\n",
       "0  MeNyrbie PhilGahan Chrisitv httpstcoiFzFAnPa a...          1   \n",
       "1  advice Talk to your neighbours family to excha...          2   \n",
       "2  Coronavirus Australia Woolworths to give elder...          2   \n",
       "3  My food stock is not the only one which is emp...          2   \n",
       "4  Me ready to go at supermarket during the COVID...          0   \n",
       "\n",
       "                           OriginalTweetPreprocessed  \n",
       "0  MeNyrbie PhilGahan Chrisitv httpstcoiFzFAnPa h...  \n",
       "1  advice Talk neighbours family exchange phone n...  \n",
       "2  Coronavirus Australia Woolworths give elderly ...  \n",
       "3  My food stock one emptyPLEASE dont panic THERE...  \n",
       "4  Me ready go supermarket COVID outbreakNot Im p...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b326e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(df['OriginalTweetPreprocessed'].values, tf.string),\n",
    "            tf.cast(df['Sentiment'].values, tf.int64)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(df_test['OriginalTweetPreprocessed'].values, tf.string),\n",
    "            tf.cast(df_test['Sentiment'].values, tf.int64)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1fced4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4c430fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(df.OriginalTweetPreprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57dc4db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "#         input_dim=61,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4660793a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.string, name='text_vectorization_1_input'), name='text_vectorization_1_input', description=\"created by layer 'text_vectorization_1_input'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.string, name='text_vectorization_1_input'), name='text_vectorization_1_input', description=\"created by layer 'text_vectorization_1_input'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.string, name='text_vectorization_1_input'), name='text_vectorization_1_input', description=\"created by layer 'text_vectorization_1_input'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.string, name='text_vectorization_1_input'), name='text_vectorization_1_input', description=\"created by layer 'text_vectorization_1_input'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/79 [============================>.] - ETA: 0s - loss: 0.5970 - accuracy: 0.3886WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.string, name='text_vectorization_1_input'), name='text_vectorization_1_input', description=\"created by layer 'text_vectorization_1_input'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.string, name='text_vectorization_1_input'), name='text_vectorization_1_input', description=\"created by layer 'text_vectorization_1_input'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 8s 50ms/step - loss: 0.5943 - accuracy: 0.3860 - val_loss: 0.4287 - val_accuracy: 0.1557\n",
      "Epoch 2/30\n",
      "79/79 [==============================] - 2s 26ms/step - loss: 0.1186 - accuracy: 0.1772 - val_loss: 0.0879 - val_accuracy: 0.1573\n",
      "Epoch 3/30\n",
      "79/79 [==============================] - 2s 26ms/step - loss: -0.1589 - accuracy: 0.1782 - val_loss: 0.0378 - val_accuracy: 0.1573\n",
      "Epoch 4/30\n",
      "79/79 [==============================] - 2s 27ms/step - loss: -0.4897 - accuracy: 0.1800 - val_loss: -0.3384 - val_accuracy: 0.1589\n",
      "Epoch 5/30\n",
      "79/79 [==============================] - 2s 28ms/step - loss: -1.6013 - accuracy: 0.2044 - val_loss: -1.8855 - val_accuracy: 0.2062\n",
      "Epoch 6/30\n",
      "79/79 [==============================] - 2s 28ms/step - loss: -4.6165 - accuracy: 0.2786 - val_loss: -4.6857 - val_accuracy: 0.2891\n",
      "Epoch 7/30\n",
      "79/79 [==============================] - 2s 29ms/step - loss: -9.1999 - accuracy: 0.3220 - val_loss: -7.4784 - val_accuracy: 0.2693\n",
      "Epoch 8/30\n",
      "79/79 [==============================] - 3s 34ms/step - loss: -13.9453 - accuracy: 0.3356 - val_loss: -10.0739 - val_accuracy: 0.2625\n",
      "Epoch 9/30\n",
      "79/79 [==============================] - 3s 36ms/step - loss: -18.9430 - accuracy: 0.3402 - val_loss: -14.1087 - val_accuracy: 0.3021\n",
      "Epoch 10/30\n",
      "79/79 [==============================] - 3s 36ms/step - loss: -24.4055 - accuracy: 0.3492 - val_loss: -16.9505 - val_accuracy: 0.2859\n",
      "Epoch 11/30\n",
      "79/79 [==============================] - 3s 35ms/step - loss: -29.9977 - accuracy: 0.3534 - val_loss: -20.8404 - val_accuracy: 0.3120\n",
      "Epoch 12/30\n",
      "79/79 [==============================] - 3s 36ms/step - loss: -35.9946 - accuracy: 0.3582 - val_loss: -24.6528 - val_accuracy: 0.3094\n",
      "Epoch 13/30\n",
      "79/79 [==============================] - 3s 36ms/step - loss: -42.2720 - accuracy: 0.3616 - val_loss: -27.8879 - val_accuracy: 0.3021\n",
      "Epoch 14/30\n",
      "79/79 [==============================] - 3s 35ms/step - loss: -48.6226 - accuracy: 0.3622 - val_loss: -30.9534 - val_accuracy: 0.2969\n",
      "Epoch 15/30\n",
      "79/79 [==============================] - 3s 35ms/step - loss: -55.2716 - accuracy: 0.3630 - val_loss: -36.0563 - val_accuracy: 0.3167\n",
      "Epoch 16/30\n",
      "79/79 [==============================] - 3s 35ms/step - loss: -61.9268 - accuracy: 0.3622 - val_loss: -39.3654 - val_accuracy: 0.3141\n",
      "Epoch 17/30\n",
      "79/79 [==============================] - 3s 36ms/step - loss: -69.1509 - accuracy: 0.3658 - val_loss: -42.1161 - val_accuracy: 0.2958\n",
      "Epoch 18/30\n",
      "79/79 [==============================] - 3s 35ms/step - loss: -73.6996 - accuracy: 0.3754 - val_loss: -46.7083 - val_accuracy: 0.3083\n",
      "Epoch 19/30\n",
      "79/79 [==============================] - 3s 36ms/step - loss: -80.7782 - accuracy: 0.3688 - val_loss: -53.3780 - val_accuracy: 0.3276\n",
      "Epoch 20/30\n",
      "79/79 [==============================] - 3s 36ms/step - loss: -90.6640 - accuracy: 0.3670 - val_loss: -55.1138 - val_accuracy: 0.3031\n",
      "Epoch 21/30\n",
      "79/79 [==============================] - 3s 36ms/step - loss: -99.2203 - accuracy: 0.3686 - val_loss: -59.1184 - val_accuracy: 0.3042\n",
      "Epoch 22/30\n",
      "79/79 [==============================] - 3s 36ms/step - loss: -106.2761 - accuracy: 0.3690 - val_loss: -66.3303 - val_accuracy: 0.3255\n",
      "Epoch 23/30\n",
      "79/79 [==============================] - 3s 36ms/step - loss: -115.4093 - accuracy: 0.3670 - val_loss: -70.2704 - val_accuracy: 0.3135\n",
      "Epoch 24/30\n",
      "79/79 [==============================] - 3s 36ms/step - loss: -125.3678 - accuracy: 0.3728 - val_loss: -74.9625 - val_accuracy: 0.3109\n",
      "Epoch 25/30\n",
      "79/79 [==============================] - 3s 37ms/step - loss: -132.8322 - accuracy: 0.3728 - val_loss: -78.3662 - val_accuracy: 0.3052\n",
      "Epoch 26/30\n",
      "79/79 [==============================] - 3s 35ms/step - loss: -141.1266 - accuracy: 0.3704 - val_loss: -85.1495 - val_accuracy: 0.3104\n",
      "Epoch 27/30\n",
      "79/79 [==============================] - 3s 37ms/step - loss: -152.4471 - accuracy: 0.3708 - val_loss: -89.8832 - val_accuracy: 0.3115\n",
      "Epoch 28/30\n",
      "79/79 [==============================] - 3s 36ms/step - loss: -159.8265 - accuracy: 0.3766 - val_loss: -95.6208 - val_accuracy: 0.3094\n",
      "Epoch 29/30\n",
      "79/79 [==============================] - 3s 35ms/step - loss: -171.3490 - accuracy: 0.3706 - val_loss: -100.0581 - val_accuracy: 0.3141\n",
      "Epoch 30/30\n",
      "79/79 [==============================] - 3s 36ms/step - loss: -181.4397 - accuracy: 0.3728 - val_loss: -102.7038 - val_accuracy: 0.3021\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=30,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8097de79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 1s 12ms/step - loss: -104.1602 - accuracy: 0.2983\n",
      "Test Loss: -104.16022491455078\n",
      "Test Accuracy: 0.2983148992061615\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
